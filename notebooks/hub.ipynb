{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MultiViewRNN\n",
    "from utils import _load_config\n",
    "\n",
    "config_file= _load_config()\n",
    "model=MultiViewRNN(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path='/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/train_aligned_dataset'\n",
    "from dataset import get_dev_loader,get_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[0;32m----> 2\u001b[0m train_loader\u001b[38;5;241m=\u001b[39m\u001b[43mget_train_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_reduced_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_file\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_batch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_file\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss_fn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/acoustic_stuff/hindi-acoustic-word-embedding/dataset.py:271\u001b[0m, in \u001b[0;36mget_train_loader\u001b[0;34m(csv_file, batch_size, loss_fn)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_train_loader\u001b[39m(csv_file,batch_size,loss_fn):\n\u001b[0;32m--> 271\u001b[0m     dataset\u001b[38;5;241m=\u001b[39m\u001b[43mMultiViewTrainDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m     loader\u001b[38;5;241m=\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[1;32m    274\u001b[0m                       collate_fn\u001b[38;5;241m=\u001b[39mtrain_collate_fn)\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loader\n",
      "File \u001b[0;32m~/acoustic_stuff/hindi-acoustic-word-embedding/dataset.py:75\u001b[0m, in \u001b[0;36mMultiViewTrainDataset.__init__\u001b[0;34m(self, csv_file, loss_fn, n_mfcc)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,csv_file,loss_fn,n_mfcc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m):\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn\u001b[38;5;241m=\u001b[39mloss_fn\n",
      "\u001b[0;31mTypeError\u001b[0m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": [
    "import os \n",
    "train_loader=get_train_loader(csv_file=os.path.join(root_path,'train_reduced_data.csv'),batch_size=config_file['train_batch_size'],loss_fn=config_file['loss_fn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model_hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 69.2M/69.2M [00:08<00:00, 8.57MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SamagraDataGov/model_hub/commit/1a13d70d541697e8fb6e432e2a03cb29d8da18dd', commit_message='Push model using huggingface_hub.', commit_description='', oid='1a13d70d541697e8fb6e432e2a03cb29d8da18dd', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"model_hub\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiViewRNN(\n",
       "  (net): ModuleDict(\n",
       "    (view1): RNN_default(\n",
       "      (rnn): LSTM(39, 512, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "    )\n",
       "    (view2): RNN_default(\n",
       "      (rnn): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.from_pretrained(\"model_hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('net.view1.rnn.weight_ih_l0',\n",
       "              tensor([[-0.1375, -0.1440,  0.0121,  ..., -0.1096, -0.1311, -0.0047],\n",
       "                      [-0.0234, -0.1309,  0.0947,  ...,  0.1179, -0.0131,  0.0127],\n",
       "                      [-0.0395,  0.0752, -0.0666,  ...,  0.0792, -0.0674,  0.0679],\n",
       "                      ...,\n",
       "                      [-0.0957,  0.0877, -0.0648,  ...,  0.1553, -0.0203,  0.0662],\n",
       "                      [ 0.1288,  0.1130, -0.1050,  ..., -0.1383,  0.0947, -0.0648],\n",
       "                      [-0.1308, -0.0214, -0.0588,  ..., -0.0297,  0.1540,  0.0804]])),\n",
       "             ('net.view1.rnn.weight_hh_l0',\n",
       "              tensor([[ 3.3391e-02,  9.3503e-03, -3.0432e-02,  ..., -3.0405e-02,\n",
       "                       -8.0260e-03, -9.5078e-03],\n",
       "                      [-3.5993e-02, -1.7496e-02, -1.2675e-02,  ..., -1.0455e-02,\n",
       "                       -5.5603e-02,  6.7202e-03],\n",
       "                      [-6.1561e-05, -3.1219e-02, -1.8498e-03,  ...,  2.7838e-02,\n",
       "                       -1.2237e-02, -2.2960e-02],\n",
       "                      ...,\n",
       "                      [ 2.9417e-02,  4.5187e-02, -1.5401e-02,  ...,  8.1720e-03,\n",
       "                       -1.7008e-02, -1.3143e-02],\n",
       "                      [ 3.3362e-02, -6.2936e-02,  2.7208e-02,  ...,  2.4565e-02,\n",
       "                        3.9608e-03, -8.2623e-03],\n",
       "                      [-9.8475e-03, -4.3499e-03,  2.6100e-03,  ...,  1.2190e-02,\n",
       "                       -4.1128e-03,  1.4397e-02]])),\n",
       "             ('net.view1.rnn.bias_ih_l0',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view1.rnn.bias_hh_l0',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view1.rnn.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.0356, -0.1381,  0.1020,  ..., -0.1392,  0.1269,  0.0043],\n",
       "                      [-0.0559,  0.0028, -0.1179,  ...,  0.1202,  0.0120, -0.0739],\n",
       "                      [ 0.0266, -0.0606, -0.0898,  ..., -0.1081, -0.0702, -0.0699],\n",
       "                      ...,\n",
       "                      [-0.0420,  0.0063,  0.0396,  ..., -0.0880, -0.0003, -0.0049],\n",
       "                      [ 0.0421, -0.1292, -0.1573,  ..., -0.0878, -0.0921, -0.1204],\n",
       "                      [-0.0096, -0.0454,  0.0352,  ...,  0.1127, -0.1584, -0.0201]])),\n",
       "             ('net.view1.rnn.weight_hh_l0_reverse',\n",
       "              tensor([[ 0.0184,  0.0214,  0.0119,  ..., -0.0004,  0.0232, -0.0199],\n",
       "                      [ 0.0015, -0.0062,  0.0154,  ...,  0.0027,  0.0218,  0.0625],\n",
       "                      [-0.0146, -0.0367,  0.0179,  ..., -0.0236,  0.0063,  0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0110, -0.0054, -0.0007,  ...,  0.0519, -0.0027, -0.0023],\n",
       "                      [-0.0045,  0.0108,  0.0195,  ...,  0.0006,  0.0024,  0.0067],\n",
       "                      [-0.0402,  0.0255,  0.0194,  ..., -0.0289,  0.0008,  0.0284]])),\n",
       "             ('net.view1.rnn.bias_ih_l0_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view1.rnn.bias_hh_l0_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view1.rnn.weight_ih_l1',\n",
       "              tensor([[-0.0051,  0.0026, -0.0249,  ...,  0.0230, -0.0182,  0.0150],\n",
       "                      [ 0.0301,  0.0225,  0.0250,  ...,  0.0205, -0.0262,  0.0082],\n",
       "                      [ 0.0143, -0.0030,  0.0041,  ...,  0.0031, -0.0225, -0.0008],\n",
       "                      ...,\n",
       "                      [-0.0133,  0.0188, -0.0058,  ..., -0.0048,  0.0136,  0.0067],\n",
       "                      [ 0.0279, -0.0077, -0.0003,  ...,  0.0055, -0.0252, -0.0111],\n",
       "                      [-0.0130, -0.0089,  0.0264,  ...,  0.0225,  0.0204,  0.0172]])),\n",
       "             ('net.view1.rnn.weight_hh_l1',\n",
       "              tensor([[ 0.0446,  0.0182,  0.0058,  ...,  0.0293, -0.0110, -0.0168],\n",
       "                      [ 0.0067,  0.0389, -0.0005,  ...,  0.0032,  0.0153,  0.0464],\n",
       "                      [ 0.0119, -0.0473,  0.0325,  ...,  0.0054, -0.0051,  0.0162],\n",
       "                      ...,\n",
       "                      [ 0.0235, -0.0382, -0.0067,  ...,  0.0071,  0.0048,  0.0021],\n",
       "                      [ 0.0357,  0.0164,  0.0332,  ...,  0.0717,  0.0303, -0.0275],\n",
       "                      [-0.0355,  0.0277,  0.0273,  ...,  0.0204, -0.0148, -0.0425]])),\n",
       "             ('net.view1.rnn.bias_ih_l1',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view1.rnn.bias_hh_l1',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view1.rnn.weight_ih_l1_reverse',\n",
       "              tensor([[-0.0270,  0.0197, -0.0092,  ...,  0.0218, -0.0183, -0.0210],\n",
       "                      [ 0.0099,  0.0049, -0.0040,  ..., -0.0301,  0.0040, -0.0283],\n",
       "                      [-0.0220, -0.0252, -0.0095,  ..., -0.0041,  0.0094, -0.0203],\n",
       "                      ...,\n",
       "                      [ 0.0250,  0.0269, -0.0070,  ...,  0.0248,  0.0130,  0.0304],\n",
       "                      [-0.0030,  0.0004, -0.0239,  ..., -0.0026, -0.0119,  0.0181],\n",
       "                      [-0.0296,  0.0267,  0.0304,  ...,  0.0086, -0.0028, -0.0224]])),\n",
       "             ('net.view1.rnn.weight_hh_l1_reverse',\n",
       "              tensor([[ 0.0492, -0.0181,  0.0010,  ...,  0.0192, -0.0226,  0.0077],\n",
       "                      [ 0.0166, -0.0400,  0.0091,  ...,  0.0014, -0.0145, -0.0153],\n",
       "                      [-0.0171,  0.0007, -0.0156,  ...,  0.0235,  0.0050, -0.0103],\n",
       "                      ...,\n",
       "                      [ 0.0138, -0.0033, -0.0622,  ..., -0.0169, -0.0548,  0.0402],\n",
       "                      [ 0.0152, -0.0011,  0.0185,  ..., -0.0295,  0.0048, -0.0009],\n",
       "                      [-0.0115, -0.0108, -0.0240,  ..., -0.0375, -0.0114, -0.0307]])),\n",
       "             ('net.view1.rnn.bias_ih_l1_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view1.rnn.bias_hh_l1_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view2.rnn.weight_ih_l0',\n",
       "              tensor([[-0.0115, -0.0148, -0.0334,  ..., -0.0911,  0.0684,  0.0023],\n",
       "                      [-0.0150, -0.0198,  0.0806,  ..., -0.0200, -0.0602, -0.0833],\n",
       "                      [ 0.0193, -0.0275,  0.0212,  ..., -0.1043,  0.0585, -0.0684],\n",
       "                      ...,\n",
       "                      [ 0.0231,  0.0376,  0.0799,  ...,  0.0678,  0.0138, -0.1053],\n",
       "                      [-0.0498,  0.0089, -0.0147,  ...,  0.0639, -0.0338,  0.0817],\n",
       "                      [ 0.0319,  0.0762,  0.0975,  ...,  0.1026, -0.0342,  0.0705]])),\n",
       "             ('net.view2.rnn.weight_hh_l0',\n",
       "              tensor([[-0.0185, -0.0063,  0.0154,  ..., -0.0290,  0.0244, -0.0246],\n",
       "                      [-0.0416, -0.0103,  0.0297,  ...,  0.0499, -0.0037,  0.0031],\n",
       "                      [-0.0169,  0.0287,  0.0065,  ...,  0.0126,  0.0042, -0.0261],\n",
       "                      ...,\n",
       "                      [-0.0194, -0.0173,  0.0222,  ...,  0.0653,  0.0237,  0.0146],\n",
       "                      [ 0.0334,  0.0208,  0.0276,  ..., -0.0264,  0.0025, -0.0016],\n",
       "                      [ 0.0181,  0.0261,  0.0206,  ...,  0.0033, -0.0129,  0.0181]])),\n",
       "             ('net.view2.rnn.bias_ih_l0',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view2.rnn.bias_hh_l0',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view2.rnn.weight_ih_l0_reverse',\n",
       "              tensor([[ 0.0530, -0.0385,  0.0222,  ..., -0.0681,  0.1008, -0.0037],\n",
       "                      [ 0.0221, -0.1045,  0.0755,  ..., -0.0721,  0.0857, -0.0584],\n",
       "                      [ 0.0110, -0.0457, -0.0517,  ..., -0.0926,  0.1010,  0.0062],\n",
       "                      ...,\n",
       "                      [-0.1025,  0.0128, -0.0279,  ...,  0.0757, -0.1007,  0.0562],\n",
       "                      [ 0.0469, -0.0402,  0.1056,  ..., -0.0794, -0.0311, -0.0971],\n",
       "                      [ 0.0178, -0.0334,  0.1040,  ...,  0.0581, -0.0448,  0.0955]])),\n",
       "             ('net.view2.rnn.weight_hh_l0_reverse',\n",
       "              tensor([[-0.0309, -0.0115, -0.0160,  ..., -0.0267, -0.0256, -0.0416],\n",
       "                      [ 0.0274,  0.0422, -0.0107,  ..., -0.0230, -0.0255, -0.0251],\n",
       "                      [ 0.0097,  0.0111,  0.0166,  ..., -0.0006, -0.0154, -0.0221],\n",
       "                      ...,\n",
       "                      [-0.0334, -0.0410, -0.0351,  ...,  0.0041, -0.0122, -0.0328],\n",
       "                      [-0.0065,  0.0102,  0.0101,  ..., -0.0089,  0.0224,  0.0204],\n",
       "                      [ 0.0078, -0.0107,  0.0053,  ...,  0.0184,  0.0060, -0.0150]])),\n",
       "             ('net.view2.rnn.bias_ih_l0_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view2.rnn.bias_hh_l0_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view2.rnn.weight_ih_l1',\n",
       "              tensor([[-0.0054,  0.0085,  0.0241,  ..., -0.0060,  0.0250,  0.0160],\n",
       "                      [ 0.0067,  0.0295,  0.0271,  ...,  0.0254,  0.0108,  0.0008],\n",
       "                      [-0.0083, -0.0063, -0.0232,  ..., -0.0205, -0.0308,  0.0255],\n",
       "                      ...,\n",
       "                      [ 0.0280, -0.0024, -0.0160,  ...,  0.0225, -0.0183, -0.0094],\n",
       "                      [-0.0049,  0.0190,  0.0183,  ..., -0.0025, -0.0188, -0.0282],\n",
       "                      [-0.0268, -0.0224, -0.0196,  ..., -0.0301, -0.0288, -0.0145]])),\n",
       "             ('net.view2.rnn.weight_hh_l1',\n",
       "              tensor([[-0.0089,  0.0177,  0.0087,  ..., -0.0190, -0.0485,  0.0842],\n",
       "                      [ 0.0137, -0.0061, -0.0333,  ...,  0.0068,  0.0103, -0.0194],\n",
       "                      [ 0.0170, -0.0107,  0.0423,  ...,  0.0116,  0.0205,  0.0199],\n",
       "                      ...,\n",
       "                      [ 0.0107, -0.0324, -0.0490,  ..., -0.0241,  0.0284,  0.0268],\n",
       "                      [ 0.0262,  0.0047,  0.0105,  ...,  0.0121,  0.0360, -0.0012],\n",
       "                      [-0.0303, -0.0168,  0.0162,  ..., -0.0140, -0.0219, -0.0123]])),\n",
       "             ('net.view2.rnn.bias_ih_l1',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view2.rnn.bias_hh_l1',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view2.rnn.weight_ih_l1_reverse',\n",
       "              tensor([[ 0.0312, -0.0097,  0.0281,  ...,  0.0143, -0.0174, -0.0312],\n",
       "                      [-0.0154,  0.0058, -0.0064,  ...,  0.0074, -0.0071, -0.0021],\n",
       "                      [-0.0130, -0.0265, -0.0265,  ...,  0.0249,  0.0052,  0.0083],\n",
       "                      ...,\n",
       "                      [-0.0218,  0.0106,  0.0247,  ..., -0.0133, -0.0094, -0.0035],\n",
       "                      [-0.0016,  0.0001,  0.0227,  ...,  0.0277,  0.0067, -0.0164],\n",
       "                      [ 0.0213, -0.0120, -0.0083,  ..., -0.0167,  0.0075,  0.0107]])),\n",
       "             ('net.view2.rnn.weight_hh_l1_reverse',\n",
       "              tensor([[ 1.8834e-02, -1.6696e-03, -2.2327e-02,  ...,  6.3027e-03,\n",
       "                       -2.1762e-02, -8.3842e-03],\n",
       "                      [ 2.3822e-02,  7.7108e-03, -1.3776e-02,  ..., -1.0757e-03,\n",
       "                        1.9213e-02, -5.6815e-03],\n",
       "                      [-2.9547e-02, -3.4732e-02, -4.7582e-02,  ..., -5.7389e-03,\n",
       "                       -3.2867e-02,  5.6750e-03],\n",
       "                      ...,\n",
       "                      [ 1.4139e-03, -8.8370e-03, -5.5362e-02,  ...,  2.0804e-02,\n",
       "                       -4.2364e-02,  7.6718e-03],\n",
       "                      [-3.7771e-02, -9.1517e-04, -1.6559e-02,  ...,  2.5968e-02,\n",
       "                       -4.6199e-02,  1.3432e-02],\n",
       "                      [ 3.1193e-02,  7.0350e-05, -2.1948e-02,  ..., -3.3475e-03,\n",
       "                        1.7968e-02,  2.9495e-02]])),\n",
       "             ('net.view2.rnn.bias_ih_l1_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('net.view2.rnn.bias_hh_l1_reverse',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "optimizer = optim.Adam(model.parameters(), lr=config_file[\"lr\"], weight_decay=config_file['weight_decay'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"model_hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import load_checkpoint\n",
    "\n",
    "step=load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli] in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (0.23.3)\n",
      "Collecting huggingface_hub[cli]\n",
      "  Obtaining dependency information for huggingface_hub[cli] from https://files.pythonhosted.org/packages/69/d6/73f9d1b7c4da5f0544bc17680d0fa9932445423b90cd38e1ee77d001a4f5/huggingface_hub-0.23.4-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from huggingface_hub[cli]) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from huggingface_hub[cli]) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from huggingface_hub[cli]) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from huggingface_hub[cli]) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from huggingface_hub[cli]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from huggingface_hub[cli]) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from huggingface_hub[cli]) (4.12.1)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
      "  Obtaining dependency information for InquirerPy==0.3.4 from https://files.pythonhosted.org/packages/ce/ff/3b59672c47c6284e8005b42e84ceba13864aa0f39f067c973d1af02f5d91/InquirerPy-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
      "  Obtaining dependency information for pfzy<0.4.0,>=0.3.1 from https://files.pythonhosted.org/packages/8c/d7/8ff98376b1acc4503253b685ea09981697385ce344d4e3935c2af49e044d/pfzy-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.47)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from requests->huggingface_hub[cli]) (2024.6.2)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
      "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m974.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: pfzy, InquirerPy, huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.23.3\n",
      "    Uninstalling huggingface-hub-0.23.3:\n",
      "      Successfully uninstalled huggingface-hub-0.23.3\n",
      "Successfully installed InquirerPy-0.3.4 huggingface_hub-0.23.4 pfzy-0.3.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "\n",
      "positional arguments:\n",
      "  {env,login,whoami,logout,repo,upload,download,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache,tag}\n",
      "                        huggingface-cli command helpers\n",
      "    env                 Print information about the environment.\n",
      "    login               Log in using a token from\n",
      "                        huggingface.co/settings/tokens\n",
      "    whoami              Find out which huggingface.co account you are logged\n",
      "                        in as.\n",
      "    logout              Log out\n",
      "    repo                {create} Commands to interact with your huggingface.co\n",
      "                        repos.\n",
      "    upload              Upload a file or a folder to a repo on the Hub\n",
      "    download            Download files from the Hub\n",
      "    lfs-enable-largefiles\n",
      "                        Configure your repository to enable upload of files >\n",
      "                        5GB.\n",
      "    scan-cache          Scan cache directory.\n",
      "    delete-cache        Delete revisions from the cache directory.\n",
      "    tag                 (create, list, delete) tags for a repo in the hub\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'hindi-acoustic-embedding-dataset'...\n",
      "remote: Enumerating objects: 3, done.\u001b[K\n",
      "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (3/3), 1.15 KiB | 1.15 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://huggingface.co/datasets/SamagraDataGov/hindi-acoustic-embedding-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated git hooks.\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "! git lfs install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated git hooks.\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "! git lfs install --skip-smudge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking \"hindi-acoustic-embedding-dataset\"\n"
     ]
    }
   ],
   "source": [
    "! git lfs track hindi-acoustic-embedding-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local repo set up for largefiles\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli lfs-enable-largefiles ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('hindi-acoustic-embedding-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd hindi-acoustic-embedding-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/hindi-acoustic-embedding-dataset\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 2 commits.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "! git status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 2 commits.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "! git commit -m \"last commit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  .git  .gitattributes  train_aligned_dataset\n"
     ]
    }
   ],
   "source": [
    "! ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32mmain\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! git branch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 9, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 9 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (9/9), 2.96 KiB | 2.96 MiB/s, done.\n",
      "From https://huggingface.co/datasets/SamagraDataGov/hindi-acoustic-embedding-dataset\n",
      "   0a88897d4..e77366fb5  main       -> origin/main\n",
      "\u001b[33mhint: You have divergent branches and need to specify how to reconcile them.\u001b[m\n",
      "\u001b[33mhint: You can do so by running one of the following commands sometime before\u001b[m\n",
      "\u001b[33mhint: your next pull:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase false  # merge (the default strategy)\u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase true   # rebase\u001b[m\n",
      "\u001b[33mhint:   git config pull.ff only       # fast-forward only\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: You can replace \"git config\" with \"git config --global\" to set a default\u001b[m\n",
      "\u001b[33mhint: preference for all repositories. You can also pass --rebase, --no-rebase,\u001b[m\n",
      "\u001b[33mhint: or --ff-only on the command line to override the configured default per\u001b[m\n",
      "\u001b[33mhint: invocation.\u001b[m\n",
      "fatal: Need to specify how to reconcile divergent branches.\n"
     ]
    }
   ],
   "source": [
    "! git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin\thttps://huggingface.co/datasets/SamagraDataGov/hindi-acoustic-embedding-dataset (fetch)\n",
      "origin\thttps://huggingface.co/datasets/SamagraDataGov/hindi-acoustic-embedding-dataset (push)\n"
     ]
    }
   ],
   "source": [
    "! git remote -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: You have not concluded your merge (MERGE_HEAD exists).\n",
      "\u001b[33mhint: Please, commit your changes before merging.\u001b[m\n",
      "fatal: Exiting because of unfinished merge.\n"
     ]
    }
   ],
   "source": [
    "! git pull https://huggingface.co/datasets/SamagraDataGov/hindi-acoustic-embedding-dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch and 'origin/main' have diverged,\n",
      "and have 2 and 3 different commits each, respectively.\n",
      "  (use \"git pull\" to merge the remote branch into yours)\n",
      "\n",
      "All conflicts fixed but you are still merging.\n",
      "  (use \"git commit\" to conclude merge)\n",
      "\n",
      "Changes to be committed:\n",
      "\t\u001b[32mnew file:   README.md\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! git status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main f963a9825] readme pulled\n"
     ]
    }
   ],
   "source": [
    "! git commit -m \"readme pulled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading LFS objects: 100% (80211/80211), 1.7 GB | 0 B/s, done.                \n",
      "Enumerating objects: 80228, done.\n",
      "Counting objects: 100% (80227/80227), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (80221/80221), done.\n",
      "Writing objects: 100% (80224/80224), 18.36 MiB | 7.75 MiB/s, done.\n",
      "Total 80224 (delta 5), reused 80219 (delta 3), pack-reused 0\n",
      "remote: Resolving deltas: 100% (5/5), completed with 1 local object.\u001b[K\n",
      "remote: \u001b[31m-------------------------------------------------------------------------\u001b[0m\u001b[K\n",
      "remote: \u001b[31mYour push was rejected because it contains files larger than 10 MiB.\u001b[0m\u001b[K\n",
      "remote: \u001b[31mPlease use https://git-lfs.github.com/ to store large files.\u001b[0m\u001b[K\n",
      "remote: \u001b[31mSee also: https://hf.co/docs/hub/repositories-getting-started#terminal\u001b[0m\u001b[K\n",
      "remote: \u001b[31m\u001b[0m\u001b[K\n",
      "remote: \u001b[31mOffending files:\u001b[0m\u001b[K\n",
      "remote: \u001b[31m  - train_aligned_dataset/metadata.csv (ref: refs/heads/main)\u001b[0m\u001b[K\n",
      "remote: \u001b[31m-------------------------------------------------------------------------\u001b[0m\u001b[K\n",
      "To https://huggingface.co/datasets/SamagraDataGov/hindi-acoustic-embedding-dataset\n",
      " \u001b[31m! [remote rejected]    \u001b[m main -> main (pre-receive hook declined)\n",
      "\u001b[31merror: failed to push some refs to 'https://huggingface.co/datasets/SamagraDataGov/hindi-acoustic-embedding-dataset'\n",
      "\u001b[m"
     ]
    }
   ],
   "source": [
    "! git push "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
