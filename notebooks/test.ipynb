{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MultiViewRNN\n",
    "from utils import _load_config,load_checkpoint,_load_vocab\n",
    "import torch.optim as optim \n",
    "\n",
    "config_file=_load_config()\n",
    "model=MultiViewRNN(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/acoustic_stuff/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config_file[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "def load_checkpoint(filepath, model, optimizer=None):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    step = checkpoint['step']\n",
    "    return model, optimizer, step\n",
    "\n",
    "checkpoint_path='/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/checkpoints2/checkpoint_epoch_97.pth.tar'\n",
    "model,optimizer,step=load_checkpoint(checkpoint_path,model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiViewRNN(\n",
       "  (net): ModuleDict(\n",
       "    (view1): RNN_default(\n",
       "      (rnn): LSTM(39, 512, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "    )\n",
       "    (view2): RNN_default(\n",
       "      (rnn): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0003\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32219"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiViewRNN(\n",
       "  (net): ModuleDict(\n",
       "    (view1): RNN_default(\n",
       "      (rnn): LSTM(39, 512, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "    )\n",
       "    (view2): RNN_default(\n",
       "      (rnn): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict=_load_vocab()\n",
    "def char_to_index(transcript):\n",
    "    one_hot=torch.zeros(len(transcript),len(vocab_dict))\n",
    "    for i,char in enumerate(transcript):\n",
    "        one_hot[i,vocab_dict[char]]=1 \n",
    "        \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np \n",
    "def compute_mfcc(audio_path,n_mfcc=13):\n",
    "    y,sr=librosa.load(audio_path)\n",
    "\n",
    "    n_fft = min(2048, len(y))\n",
    "    hop_length = n_fft // 4\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr,n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    width = min(9, mfccs.shape[1])\n",
    "    if width < 3:\n",
    "        width = 3\n",
    "        \n",
    "    width = min(width, mfccs.shape[1])\n",
    "\n",
    "    if width % 2 == 0:\n",
    "        width -= 1\n",
    "\n",
    "        \n",
    "    delta1=librosa.feature.delta(mfccs,order=1,width=width)\n",
    "    delta2=librosa.feature.delta(mfccs,order=2,width=width)\n",
    "\n",
    "    mfccs_combined=np.concatenate((mfccs,delta1,delta2),axis=0)\n",
    "    \n",
    "\n",
    "    return torch.tensor(mfccs_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_audio(audio_path):\n",
    "    \n",
    "    mfcc=compute_mfcc(audio_path)\n",
    "    mfcc=mfcc.view(mfcc.shape[1],mfcc.shape[0])\n",
    "    mfcc=mfcc.to(DEVICE)\n",
    "    mfcc=mfcc.unsqueeze(0)\n",
    "    return {\"view1_x1\":mfcc}\n",
    "\n",
    "def prepare_input_transcript(transcript):\n",
    "\n",
    "    one_hot=char_to_index(transcript=transcript)\n",
    "    one_hot=one_hot.to(DEVICE)\n",
    "    one_hot=one_hot.unsqueeze(0)\n",
    "\n",
    "    return {\"view2_c1\":one_hot}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path='/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/train_aligned_dataset/sampled_devset.csv'\n",
    "df=pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>transcript</th>\n",
       "      <th>duration</th>\n",
       "      <th>sampled_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4934</td>\n",
       "      <td>844424931200572-977-m_seg_1.wav</td>\n",
       "      <td>को</td>\n",
       "      <td>1.726-1.926 sec</td>\n",
       "      <td>{0: ['को', 'को'], 1: ['हो', 'का'], 2: ['है', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4013</td>\n",
       "      <td>844424930703499-329-f_seg_7.wav</td>\n",
       "      <td>हो</td>\n",
       "      <td>2.551-2.673 sec</td>\n",
       "      <td>{0: ['हो', 'हो'], 1: ['को', 'है'], 2: ['एक', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1056</td>\n",
       "      <td>844424933458787-572-m_seg_8.wav</td>\n",
       "      <td>है</td>\n",
       "      <td>2.560-2.926 sec</td>\n",
       "      <td>{0: ['है', 'है'], 1: ['हो', 'ही'], 2: ['को', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6619</td>\n",
       "      <td>844424933457774-1191-m_seg_0.wav</td>\n",
       "      <td>बैजू</td>\n",
       "      <td>0.000-1.893 sec</td>\n",
       "      <td>{0: ['बैजू'], 2: ['बैठक', 'बैंक'], 3: ['है', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17194</td>\n",
       "      <td>844424931170952-193-f_seg_2.wav</td>\n",
       "      <td>दैनिक</td>\n",
       "      <td>1.048-1.290 sec</td>\n",
       "      <td>{0: ['दैनिक'], 3: ['दोनों', 'दिनों', 'देना', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0                        audio_path transcript  \\\n",
       "0             0        4934   844424931200572-977-m_seg_1.wav         को   \n",
       "1             1        4013   844424930703499-329-f_seg_7.wav         हो   \n",
       "2             2        1056   844424933458787-572-m_seg_8.wav         है   \n",
       "3             3        6619  844424933457774-1191-m_seg_0.wav       बैजू   \n",
       "4             4       17194   844424931170952-193-f_seg_2.wav      दैनिक   \n",
       "\n",
       "          duration                                      sampled_words  \n",
       "0  1.726-1.926 sec  {0: ['को', 'को'], 1: ['हो', 'का'], 2: ['है', '...  \n",
       "1  2.551-2.673 sec  {0: ['हो', 'हो'], 1: ['को', 'है'], 2: ['एक', '...  \n",
       "2  2.560-2.926 sec  {0: ['है', 'है'], 1: ['हो', 'ही'], 2: ['को', '...  \n",
       "3  0.000-1.893 sec  {0: ['बैजू'], 2: ['बैठक', 'बैंक'], 3: ['है', '...  \n",
       "4  1.048-1.290 sec  {0: ['दैनिक'], 3: ['दोनों', 'दिनों', 'देना', '...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/train_aligned_dataset/sampled_trainset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>transcript</th>\n",
       "      <th>duration</th>\n",
       "      <th>word_length</th>\n",
       "      <th>distances</th>\n",
       "      <th>negative_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20196</td>\n",
       "      <td>844424933460441-934-m_seg_4.wav</td>\n",
       "      <td>देशविशेष</td>\n",
       "      <td>2.296-2.940 sec</td>\n",
       "      <td>8</td>\n",
       "      <td>[0, 6, 8, 8, 8, 7, 7, 9, 8, 8, 7, 8, 8, 9, 7, ...</td>\n",
       "      <td>[('844424932719302-126-m_seg_7.wav', 'दैनिक'),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11092</td>\n",
       "      <td>844424932719302-126-m_seg_7.wav</td>\n",
       "      <td>दैनिक</td>\n",
       "      <td>3.700-4.142 sec</td>\n",
       "      <td>5</td>\n",
       "      <td>[6, 0, 7, 6, 6, 7, 5, 8, 8, 6, 5, 7, 6, 8, 5, ...</td>\n",
       "      <td>[('844424933460441-934-m_seg_4.wav', 'देशविशेष...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21998</td>\n",
       "      <td>844424931188030-1133-m_seg_7.wav</td>\n",
       "      <td>मुहूर्त</td>\n",
       "      <td>3.252-3.613 sec</td>\n",
       "      <td>7</td>\n",
       "      <td>[8, 7, 0, 7, 7, 9, 6, 6, 10, 6, 7, 6, 7, 7, 6,...</td>\n",
       "      <td>[('844424933460441-934-m_seg_4.wav', 'देशविशेष...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8962</td>\n",
       "      <td>844424933563590-558-m_seg_3.wav</td>\n",
       "      <td>स्थानीय</td>\n",
       "      <td>1.933-2.537 sec</td>\n",
       "      <td>7</td>\n",
       "      <td>[8, 6, 7, 0, 5, 8, 7, 8, 10, 6, 6, 8, 6, 7, 7,...</td>\n",
       "      <td>[('844424933460441-934-m_seg_4.wav', 'देशविशेष...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>795</td>\n",
       "      <td>844424931282533-277-f_seg_9.wav</td>\n",
       "      <td>ज्यादा</td>\n",
       "      <td>3.379-3.741 sec</td>\n",
       "      <td>6</td>\n",
       "      <td>[8, 6, 7, 5, 0, 7, 6, 9, 10, 5, 6, 8, 6, 8, 7,...</td>\n",
       "      <td>[('844424933460441-934-m_seg_4.wav', 'देशविशेष...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        audio_path transcript         duration  \\\n",
       "0       20196   844424933460441-934-m_seg_4.wav   देशविशेष  2.296-2.940 sec   \n",
       "1       11092   844424932719302-126-m_seg_7.wav      दैनिक  3.700-4.142 sec   \n",
       "2       21998  844424931188030-1133-m_seg_7.wav    मुहूर्त  3.252-3.613 sec   \n",
       "3        8962   844424933563590-558-m_seg_3.wav    स्थानीय  1.933-2.537 sec   \n",
       "4         795   844424931282533-277-f_seg_9.wav     ज्यादा  3.379-3.741 sec   \n",
       "\n",
       "   word_length                                          distances  \\\n",
       "0            8  [0, 6, 8, 8, 8, 7, 7, 9, 8, 8, 7, 8, 8, 9, 7, ...   \n",
       "1            5  [6, 0, 7, 6, 6, 7, 5, 8, 8, 6, 5, 7, 6, 8, 5, ...   \n",
       "2            7  [8, 7, 0, 7, 7, 9, 6, 6, 10, 6, 7, 6, 7, 7, 6,...   \n",
       "3            7  [8, 6, 7, 0, 5, 8, 7, 8, 10, 6, 6, 8, 6, 7, 7,...   \n",
       "4            6  [8, 6, 7, 5, 0, 7, 6, 9, 10, 5, 6, 8, 6, 8, 7,...   \n",
       "\n",
       "                                    negative_samples  \n",
       "0  [('844424932719302-126-m_seg_7.wav', 'दैनिक'),...  \n",
       "1  [('844424933460441-934-m_seg_4.wav', 'देशविशेष...  \n",
       "2  [('844424933460441-934-m_seg_4.wav', 'देशविशेष...  \n",
       "3  [('844424933460441-934-m_seg_4.wav', 'देशविशेष...  \n",
       "4  [('844424933460441-934-m_seg_4.wav', 'देशविशेष...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('844424932906135-1128-f_seg_1.wav', 'पुलिस')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "import ast \n",
    "\n",
    "choice=random.choice(ast.literal_eval(df_train['negative_samples'][10])\n",
    "                    )\n",
    "choice \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "root_path=os.path.dirname(csv_path)\n",
    "\n",
    "audio_path=os.path.join(root_path,df_train['audio_path'][100])\n",
    "#transcript=choice[1]\n",
    "transcript=df_train['transcript'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_audio=prepare_input_audio(audio_path)\n",
    "input_text=prepare_input_transcript(transcript=transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_emb=model(input_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb=model(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "sim=F.cosine_similarity(audio_emb['x1'],text_emb['c1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2302], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampler import sample_dev_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18844/18844 [58:46<00:00,  5.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_sampling complete\n"
     ]
    }
   ],
   "source": [
    "path=sample_dev_words('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/train_aligned_dataset/train_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>transcript</th>\n",
       "      <th>duration</th>\n",
       "      <th>sampled_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>651</td>\n",
       "      <td>844424932602773-645-f_seg_9.wav</td>\n",
       "      <td>घंटी</td>\n",
       "      <td>2.732-3.013 sec</td>\n",
       "      <td>{0: ['घंटी', 'घंटी'], 1: ['घंटे', 'घाटी'], 2: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9070</td>\n",
       "      <td>844424930766406-261-f_seg_7.wav</td>\n",
       "      <td>के</td>\n",
       "      <td>3.498-3.699 sec</td>\n",
       "      <td>{0: ['के', 'के'], 1: ['को', 'थे'], 2: ['है', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20196</td>\n",
       "      <td>844424933460441-934-m_seg_4.wav</td>\n",
       "      <td>देशविशेष</td>\n",
       "      <td>2.296-2.940 sec</td>\n",
       "      <td>{0: ['देशविशेष'], 3: ['विशेष', 'विशेष', 'विशेष...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177</td>\n",
       "      <td>844424932581205-645-f_seg_10.wav</td>\n",
       "      <td>रवाना</td>\n",
       "      <td>3.508-3.790 sec</td>\n",
       "      <td>{0: ['रवाना', 'रवाना'], 2: ['रोकना', 'करवाया']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>844424930620027-1177-f_seg_3.wav</td>\n",
       "      <td>हिंसा</td>\n",
       "      <td>2.166-2.687 sec</td>\n",
       "      <td>{0: ['हिंसा', 'हिंसा'], 2: ['हादसा', 'हिस्सा']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18839</th>\n",
       "      <td>11964</td>\n",
       "      <td>844424931102635-229-f_seg_8.wav</td>\n",
       "      <td>अपने</td>\n",
       "      <td>2.627-2.870 sec</td>\n",
       "      <td>{0: ['अपने', 'अपने'], 1: ['सपने', 'अपनी'], 2: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18840</th>\n",
       "      <td>21575</td>\n",
       "      <td>844424933559042-558-m_seg_7.wav</td>\n",
       "      <td>लाभ</td>\n",
       "      <td>5.095-5.416 sec</td>\n",
       "      <td>{0: ['लाभ', 'लाभ'], 1: ['लाख', 'लाई'], 2: ['बा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18841</th>\n",
       "      <td>5390</td>\n",
       "      <td>844424932730849-579-f_seg_9.wav</td>\n",
       "      <td>होने</td>\n",
       "      <td>3.819-4.060 sec</td>\n",
       "      <td>{0: ['होने', 'होने'], 1: ['होते', 'होना'], 2: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18842</th>\n",
       "      <td>860</td>\n",
       "      <td>844424933525081-390-m_seg_9.wav</td>\n",
       "      <td>अपनी</td>\n",
       "      <td>7.899-8.220 sec</td>\n",
       "      <td>{0: ['अपनी', 'अपनी'], 1: ['अपने', 'अपना'], 2: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18843</th>\n",
       "      <td>15795</td>\n",
       "      <td>844424932709907-468-f_seg_1.wav</td>\n",
       "      <td>को</td>\n",
       "      <td>1.323-1.683 sec</td>\n",
       "      <td>{0: ['को', 'को'], 1: ['के', 'के'], 2: ['है', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18844 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                        audio_path transcript  \\\n",
       "0             651   844424932602773-645-f_seg_9.wav       घंटी   \n",
       "1            9070   844424930766406-261-f_seg_7.wav         के   \n",
       "2           20196   844424933460441-934-m_seg_4.wav   देशविशेष   \n",
       "3             177  844424932581205-645-f_seg_10.wav      रवाना   \n",
       "4               3  844424930620027-1177-f_seg_3.wav      हिंसा   \n",
       "...           ...                               ...        ...   \n",
       "18839       11964   844424931102635-229-f_seg_8.wav       अपने   \n",
       "18840       21575   844424933559042-558-m_seg_7.wav        लाभ   \n",
       "18841        5390   844424932730849-579-f_seg_9.wav       होने   \n",
       "18842         860   844424933525081-390-m_seg_9.wav       अपनी   \n",
       "18843       15795   844424932709907-468-f_seg_1.wav         को   \n",
       "\n",
       "              duration                                      sampled_words  \n",
       "0      2.732-3.013 sec  {0: ['घंटी', 'घंटी'], 1: ['घंटे', 'घाटी'], 2: ...  \n",
       "1      3.498-3.699 sec  {0: ['के', 'के'], 1: ['को', 'थे'], 2: ['है', '...  \n",
       "2      2.296-2.940 sec  {0: ['देशविशेष'], 3: ['विशेष', 'विशेष', 'विशेष...  \n",
       "3      3.508-3.790 sec  {0: ['रवाना', 'रवाना'], 2: ['रोकना', 'करवाया']...  \n",
       "4      2.166-2.687 sec  {0: ['हिंसा', 'हिंसा'], 2: ['हादसा', 'हिस्सा']...  \n",
       "...                ...                                                ...  \n",
       "18839  2.627-2.870 sec  {0: ['अपने', 'अपने'], 1: ['सपने', 'अपनी'], 2: ...  \n",
       "18840  5.095-5.416 sec  {0: ['लाभ', 'लाभ'], 1: ['लाख', 'लाई'], 2: ['बा...  \n",
       "18841  3.819-4.060 sec  {0: ['होने', 'होने'], 1: ['होते', 'होना'], 2: ...  \n",
       "18842  7.899-8.220 sec  {0: ['अपनी', 'अपनी'], 1: ['अपने', 'अपना'], 2: ...  \n",
       "18843  1.323-1.683 sec  {0: ['को', 'को'], 1: ['के', 'के'], 2: ['है', '...  \n",
       "\n",
       "[18844 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_dev_loader\n",
    "\n",
    "loader=get_dev_loader('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/train_aligned_dataset/sampled_trainset(2).csv',batch_size=config_file['dev_batch_size'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "from metrics import crossview_ap,crossview_corr\n",
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Evaluating Train\")\n",
    "model=model.to(DEVICE)\n",
    "model.eval()\n",
    "average_precision=0.0\n",
    "average_corr=0.0\n",
    "for idx,batch in tqdm(\n",
    "                enumerate(loader), total=len(loader), leave=False\n",
    "            ):\n",
    "            \n",
    "                mfcc=batch[\"mfcc\"]\n",
    "                mfcc=mfcc.view(-1,mfcc.shape[2],mfcc.shape[1])\n",
    "                mfcc=mfcc.to(DEVICE)\n",
    "                mfcc_input={\"view1_x1\":mfcc}\n",
    "                audio_emb=model(mfcc_input)[\"x1\"] \n",
    "\n",
    "                input_text_tensor=batch[\"sampled_one_hot\"]\n",
    "                batch_size=input_text_tensor.shape[0]\n",
    "                sampled_shape=input_text_tensor.shape[1]\n",
    "                input_text_tensor=input_text_tensor.view(input_text_tensor.shape[0]*sampled_shape,\n",
    "                                                        input_text_tensor.shape[2],\n",
    "                                                        input_text_tensor.shape[3])\n",
    "                input_text_tensor=input_text_tensor.to(DEVICE)\n",
    "                input_one_hot={\"view2_c1\":input_text_tensor}\n",
    "                out_one_hot=model(input_one_hot)[\"c1\"]\n",
    "                text_emb=out_one_hot.view(batch_size,\n",
    "                                        sampled_shape,\n",
    "                                        out_one_hot.shape[1])\n",
    "            \n",
    "                lev_distances=batch[\"lev_scores\"].to(DEVICE)\n",
    "            \n",
    "                ranked_ap=crossview_ap(audio_embedding=audio_emb,\n",
    "                                    text_embedding=text_emb,\n",
    "                                    lev_distances=lev_distances)\n",
    "                ranked_corr=crossview_corr(audio_embedding=audio_emb,\n",
    "                                        text_embedding=text_emb,\n",
    "                                        lev_distances=lev_distances)\n",
    "                average_precision+=ranked_ap\n",
    "                average_corr+=ranked_corr\n",
    "            \n",
    "\n",
    "average_precision=average_precision/len(loader)\n",
    "average_corr=average_corr/len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23969043265073126"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036177764585042446"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/589 [00:00<?, ?it/s]/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.nn.functional.pad(torch.tensor(seq), (0, 0, 0, max_seq_length - len(seq)), 'constant', padding_value)\n",
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 46\u001b[0m\n\u001b[1;32m     40\u001b[0m text_emb\u001b[38;5;241m=\u001b[39mout_one_hot\u001b[38;5;241m.\u001b[39mview(batch_size,\n\u001b[1;32m     41\u001b[0m                         sampled_shape,\n\u001b[1;32m     42\u001b[0m                         out_one_hot\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     44\u001b[0m lev_distances\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlev_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 46\u001b[0m ranked_ap\u001b[38;5;241m=\u001b[39m\u001b[43mcrossview_ap\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtext_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlev_distances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlev_distances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m ranked_corr\u001b[38;5;241m=\u001b[39mcrossview_corr(audio_embedding\u001b[38;5;241m=\u001b[39maudio_emb,\n\u001b[1;32m     50\u001b[0m                         text_embedding\u001b[38;5;241m=\u001b[39mtext_emb,\n\u001b[1;32m     51\u001b[0m                         lev_distances\u001b[38;5;241m=\u001b[39mlev_distances)\n\u001b[1;32m     52\u001b[0m average_precision\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mranked_ap\n",
      "File \u001b[0;32m~/acoustic_stuff/hindi-acoustic-word-embedding/metrics.py:29\u001b[0m, in \u001b[0;36mcrossview_ap\u001b[0;34m(audio_embedding, text_embedding, lev_distances)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrossview_ap\u001b[39m(audio_embedding,text_embedding,lev_distances):\n\u001b[1;32m     27\u001b[0m     indices\u001b[38;5;241m=\u001b[39mget_indices(audio_embedding\u001b[38;5;241m=\u001b[39maudio_embedding,text_embedding\u001b[38;5;241m=\u001b[39mtext_embedding)\n\u001b[0;32m---> 29\u001b[0m     average_precission\u001b[38;5;241m=\u001b[39m\u001b[43mranked_batch_ap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlev_distances\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m average_precission\n",
      "File \u001b[0;32m~/acoustic_stuff/hindi-acoustic-word-embedding/metrics.py:44\u001b[0m, in \u001b[0;36mranked_batch_ap\u001b[0;34m(lev_distances, cosine_ranks)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relevant_ranks\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m \n\u001b[0;32m---> 44\u001b[0m pos_indices\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrelevant_ranks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelevant_ranks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     45\u001b[0m precision_at_k\u001b[38;5;241m=\u001b[39mpos_indices\u001b[38;5;241m/\u001b[39m(relevant_ranks\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m average_precission_i\u001b[38;5;241m=\u001b[39mprecision_at_k\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m/\u001b[39mrelevant_ranks\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os \n",
    "checkpoint_path_list=os.listdir('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/checkpoints')\n",
    "root_path='/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/checkpoints'\n",
    "\n",
    "sample_path_list=checkpoint_path_list[300:]\n",
    "average_ap_list=[]\n",
    "average_corr_list=[]\n",
    "\n",
    "for i in range(len(sample_path_list)):\n",
    "\n",
    "    model=MultiViewRNN(config_file)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config_file[\"lr\"])\n",
    "    \n",
    "    model,optimizer,step=load_checkpoint(os.path.join(root_path,sample_path_list[i]),model,optimizer)\n",
    "    model=model.to(DEVICE)\n",
    "\n",
    "    print(f\"{i}\")\n",
    "    model.eval()\n",
    "    average_precision=0.0\n",
    "    average_corr=0.0\n",
    "    for idx,batch in tqdm(\n",
    "                enumerate(loader), total=len(loader), leave=False\n",
    "            ):\n",
    "            \n",
    "                mfcc=batch[\"mfcc\"]\n",
    "                mfcc=mfcc.view(-1,mfcc.shape[2],mfcc.shape[1])\n",
    "                mfcc=mfcc.to(DEVICE)\n",
    "                mfcc_input={\"view1_x1\":mfcc}\n",
    "                audio_emb=model(mfcc_input)[\"x1\"] \n",
    "\n",
    "                input_text_tensor=batch[\"sampled_one_hot\"]\n",
    "                batch_size=input_text_tensor.shape[0]\n",
    "                sampled_shape=input_text_tensor.shape[1]\n",
    "                input_text_tensor=input_text_tensor.view(input_text_tensor.shape[0]*sampled_shape,\n",
    "                                                        input_text_tensor.shape[2],\n",
    "                                                        input_text_tensor.shape[3])\n",
    "                input_text_tensor=input_text_tensor.to(DEVICE)\n",
    "                input_one_hot={\"view2_c1\":input_text_tensor}\n",
    "                out_one_hot=model(input_one_hot)[\"c1\"]\n",
    "                text_emb=out_one_hot.view(batch_size,\n",
    "                                        sampled_shape,\n",
    "                                        out_one_hot.shape[1])\n",
    "            \n",
    "                lev_distances=batch[\"lev_scores\"].to(DEVICE)\n",
    "            \n",
    "                ranked_ap=crossview_ap(audio_embedding=audio_emb,\n",
    "                                    text_embedding=text_emb,\n",
    "                                    lev_distances=lev_distances)\n",
    "                ranked_corr=crossview_corr(audio_embedding=audio_emb,\n",
    "                                        text_embedding=text_emb,\n",
    "                                        lev_distances=lev_distances)\n",
    "                average_precision+=ranked_ap\n",
    "                average_corr+=ranked_corr\n",
    "            \n",
    "\n",
    "    average_precision=average_precision/len(loader)\n",
    "    average_corr=average_corr/len(loader)\n",
    "    \n",
    "    average_ap_list.append(average_precision)\n",
    "    average_corr_list.append(average_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "x=[i for i in range(300,300+len(sample_path_list))]\n",
    "y=average_ap_list\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title('Epoch vs Train AP')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Train_AP_per_epoch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "y=average_corr_list\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title('Epoch vs Train corr')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Train_corr_per_epoch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_train_loader\n",
    "from utils import _load_config\n",
    "\n",
    "config_file=_load_config()\n",
    "\n",
    "csv_path='/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/train_aligned_dataset/sampled_trainset.csv'\n",
    "\n",
    "loader=get_train_loader(csv_file=csv_path,batch_size=config_file['train_batch_size'],loss_fn=config_file['loss_fn'])\n",
    "batch=next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['view1_x1', 'view2_c1', 'view2_c2', 'edit_distance', 'view1_x2'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['edit_distance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
