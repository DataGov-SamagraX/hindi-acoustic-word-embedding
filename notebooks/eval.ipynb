{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "train_df=pd.read_csv('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/metadata - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = train_df.sample(frac=0.3, random_state=42)\n",
    "train_set = train_df.drop(dev_df.index)\n",
    "train_set.reset_index(drop=True, inplace=True)\n",
    "dev_df.reset_index(drop=True, inplace=True)\n",
    "train_set.to_csv('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/train_set.csv', index=False)\n",
    "dev_df.to_csv('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/dev_set.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      और\n",
       "1      की\n",
       "2     गोल\n",
       "3    मछली\n",
       "4      से\n",
       "Name: transcript, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df['transcript'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0116_003_segment_1.wav</td>\n",
       "      <td>अपने</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0116_003_segment_2.wav</td>\n",
       "      <td>पेट</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0116_003_segment_7.wav</td>\n",
       "      <td>गरमगरम</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0116_003_segment_9.wav</td>\n",
       "      <td>हड़पते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0128_003_segment_0.wav</td>\n",
       "      <td>मुनिया</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              audio_path transcript\n",
       "0           1  0116_003_segment_1.wav       अपने\n",
       "1           2  0116_003_segment_2.wav        पेट\n",
       "2           7  0116_003_segment_7.wav     गरमगरम\n",
       "3           9  0116_003_segment_9.wav      हड़पते\n",
       "4          10  0128_003_segment_0.wav     मुनिया"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import pandas as pd \n",
    "import random \n",
    "import os \n",
    "\n",
    "def edit_distance(word1,word2):\n",
    "    distance=Levenshtein.distance(word1,word2)\n",
    "    return distance\n",
    "\n",
    "def sample_words_with_lev_scores(df, lev_score, word):\n",
    "    filtered_words = df[df.apply(lambda row: edit_distance(row['transcript'], word), axis=1) == lev_score]\n",
    "    \n",
    "    if len(filtered_words) >= 2:\n",
    "        sampled_words = random.sample(list(filtered_words['transcript']), 2)\n",
    "    else:\n",
    "        sampled_words = list(filtered_words['transcript'])\n",
    "    \n",
    "    return sampled_words\n",
    "\n",
    "def sample_dev_words(path):\n",
    "    df=pd.read_csv(path)\n",
    "    sampled_list=[]\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        out_dict={}\n",
    "        for j in range(len(df)):\n",
    "            lev_distance=edit_distance(df['transcript'][i],df['transcript'][j])\n",
    "            if lev_distance in out_dict:\n",
    "                out_dict[lev_distance].append(df['transcript'][j])\n",
    "            else:\n",
    "                out_dict[lev_distance]=[df['transcript'][j]]\n",
    "        \n",
    "        sampled_list.append(out_dict)\n",
    "    \"\"\"\"for i in range(len(df)):\n",
    "        score=0\n",
    "        out_dict={}\n",
    "        count_list=[]\n",
    "        while len(count_list)<8:\n",
    "            lev_score_i_words = sample_words_with_lev_scores(df, score, df['transcript'][i])\n",
    "            out_dict[score]=lev_score_i_words\n",
    "            \n",
    "            count_list+=lev_score_i_words\n",
    "            score+=1\n",
    "\n",
    "        sampled_list.append(out_dict)\"\"\"\n",
    "\n",
    "    df['sampled_words']=sampled_list\n",
    "\n",
    "    root_path=path.split('dataset/')[0] + 'dataset/'\n",
    "    save_path=os.path.join(root_path,\"sampled_devset.csv\")\n",
    "\n",
    "    df.to_csv(save_path)\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampler import sample_negatives\n",
    "\n",
    "sampled_trainset=sample_negatives('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/train_set.csv')\n",
    "sampled_dev_set=sample_dev_words('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/dev_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random, time, operator \n",
    "import os \n",
    "import torch \n",
    "from utils import _load_vocab\n",
    "from utils import load_audio\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import librosa\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import librosa.display\n",
    "import ast \n",
    "\n",
    "VOCAB_DICT=_load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiviewDevDataset(Dataset):\n",
    "\n",
    "    def __init__(self,csv_file,n_mfcc=13):\n",
    "        self.data=pd.read_csv(csv_file)\n",
    "        self.dir_path=os.path.dirname(csv_file)\n",
    "        self.vocab_dict=VOCAB_DICT\n",
    "        self.n_mfcc=n_mfcc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def char_to_idx(self,transcript):\n",
    "        \n",
    "        one_hot=torch.zeros(len(transcript),len(self.vocab_dict))\n",
    "        for i,char in enumerate(transcript):\n",
    "            one_hot[i,self.vocab_dict[char]]=1 \n",
    "        \n",
    "        return one_hot\n",
    "    \n",
    "    def compute_mfcc(self,audio_path):\n",
    "\n",
    "        y,sr=librosa.load(audio_path)\n",
    "\n",
    "        n_fft = min(2048, len(y))\n",
    "        hop_length = n_fft // 4\n",
    "\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "        width = min(9, mfccs.shape[1])\n",
    "        if width < 3:\n",
    "            width = 3\n",
    "        \n",
    "        width = min(width, mfccs.shape[1])\n",
    "\n",
    "        if width % 2 == 0:\n",
    "            width -= 1\n",
    "\n",
    "        \n",
    "        delta1=librosa.feature.delta(mfccs,order=1,width=width)\n",
    "        delta2=librosa.feature.delta(mfccs,order=2,width=width)\n",
    "\n",
    "        mfccs_combined=np.concatenate((mfccs,delta1,delta2),axis=0)\n",
    "\n",
    "        return mfccs_combined\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        audio_path_x1=self.data[\"audio_path\"][idx]\n",
    "        audio_path_x1=os.path.join(self.dir_path,str(audio_path_x1))\n",
    "\n",
    "        #mfcc \n",
    "        audio_mfcc=self.compute_mfcc(audio_path_x1)\n",
    "\n",
    "        sample_dict=ast.literal_eval(self.data[\"sampled_words\"][idx])\n",
    "        lev_scores=[]\n",
    "        for score in sample_dict.keys():\n",
    "            for _ in range(len(sample_dict[score])):\n",
    "                lev_scores.append(score)\n",
    "\n",
    "        one_hot=[]\n",
    "        for transcripts in sample_dict.values():\n",
    "            for transcript in transcripts:\n",
    "                one_hot.append(self.char_to_idx(transcript))\n",
    "        \n",
    "        output_tensor=[torch.tensor(audio_mfcc),one_hot,torch.tensor(lev_scores)]\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset=MultiviewDevDataset('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/sampled_devset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 6, 1, 8, 9])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset[0][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, batch_first=True, padding_value=0):\n",
    "    return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=batch_first, padding_value=padding_value)\n",
    "\n",
    "def pad_mfccs(mfccs, max_len):\n",
    "    padded_mfccs = []\n",
    "    for mfcc in mfccs:\n",
    "        # Padding to the right with zeros\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        padded_mfcc = torch.nn.functional.pad(mfcc, (0, pad_width), 'constant', 0)\n",
    "        padded_mfccs.append(padded_mfcc)\n",
    "    return torch.stack(padded_mfccs)\n",
    "\n",
    "def pad_list_of_lists(batch_of_sequences, padding_value=0):\n",
    "    padded_batch = []\n",
    "    for sequences in batch_of_sequences:\n",
    "        sequences = [torch.tensor(seq) for seq in sequences]\n",
    "        padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=padding_value)\n",
    "        padded_batch.append(padded_sequences)\n",
    "    \n",
    "    return torch.stack(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tensor=pad_sequence(dev_dataset[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 9, 83])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_collate_fn(batch):\n",
    "    \n",
    "    mfccs_x1=[]\n",
    "    one_hot=[]\n",
    "    lev_scores=[]\n",
    "\n",
    "    for item in batch:\n",
    "        mfcc_x1,oh,lev_score=item[0],item[1],item[2]\n",
    "        mfccs_x1.append(mfcc_x1)\n",
    "        one_hot.append(oh)\n",
    "        lev_scores.append(lev_score)\n",
    "    \n",
    "    max_mfcc_len_x1=max(mfcc.shape[1] for mfcc in mfccs_x1)\n",
    "    mfccs_x1=pad_mfccs(mfccs_x1,max_mfcc_len_x1)\n",
    "\n",
    "    one_hot=pad_list_of_lists(one_hot)\n",
    "\n",
    "    results={\"mfcc\":mfccs_x1,\"sampled_one_hot\":one_hot,\"lev_scores\":torch.stack(lev_scores)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader=DataLoader(dev_dataset, batch_size=2, collate_fn=dev_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_608917/1187922984.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequences = [torch.tensor(seq) for seq in sequences]\n"
     ]
    }
   ],
   "source": [
    "batches=iter(dev_loader)\n",
    "batch=next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 9, 83])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"sampled_one_hot\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import _load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MultiViewRNN\n",
    "\n",
    "config_file=_load_config()\n",
    "model=MultiViewRNN(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiViewRNN(\n",
       "  (net): ModuleDict(\n",
       "    (view1): RNN_default(\n",
       "      (rnn): LSTM(39, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    )\n",
       "    (view2): RNN_default(\n",
       "      (rnn): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor=batch[\"sampled_one_hot\"]\n",
    "input_tensor=input_tensor.view(input_tensor.shape[0]*18,input_tensor.shape[2],input_tensor.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 9, 83])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_one_hot={\"view2_c1\":input_tensor}\n",
    "out_one_hot=model(input_one_hot)[\"c1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 1024])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_one_hot=out_one_hot.view(2,18,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 1024])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_one_hot.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc=batch[\"mfcc\"]\n",
    "mfcc=mfcc.view(-1,mfcc.shape[2],mfcc.shape[1])\n",
    "mfcc_input={\"view1_x1\":mfcc}\n",
    "audio_emb=model(mfcc_input)[\"x1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_text_emb=out_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 1024])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_text_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_emb=audio_emb.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_emb=audio_emb.squeeze(1)\n",
    "audio_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import ranked_batch_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1024])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "normalized_audio_embeddings = F.normalize(audio_emb, p=2, dim=1)  \n",
    "normalized_text_embeddings = F.normalize(sampled_text_emb, p=2, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_audio_embeddings = normalized_audio_embeddings.unsqueeze(1)\n",
    "cosine_similarities = torch.sum(expanded_audio_embeddings * normalized_text_embeddings, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=torch.argsort(cosine_similarities,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  5,  6, 16,  1, 14,  4,  7, 15,  8,  2, 11,  3, 13, 17,  0, 10,  9],\n",
       "        [11,  7,  6,  9, 10,  8, 14, 15,  2,  3,  1, 17,  5,  0,  4, 16, 12, 13]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_score=torch.stack(batch[\"lev_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap=ranked_batch_ap(lev_score,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24358975887298584"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
