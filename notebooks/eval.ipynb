{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "train_df=pd.read_csv('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/metadata - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = train_df.sample(frac=0.3, random_state=42)\n",
    "train_set = train_df.drop(dev_df.index)\n",
    "train_set.reset_index(drop=True, inplace=True)\n",
    "dev_df.reset_index(drop=True, inplace=True)\n",
    "train_set.to_csv('/home/ubuntu/acoustic        # Pad the inner lists to have the same number of sequences\n",
    "_stuff/hindi-acoustic-word-embedding/dataset/train_set.csv', index=False)\n",
    "dev_df.to_csv('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/dev_set.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      और\n",
       "1      की\n",
       "2     गोल\n",
       "3    मछली\n",
       "4      से\n",
       "Name: transcript, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df['transcript'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0116_003_segment_1.wav</td>\n",
       "      <td>अपने</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0116_003_segment_2.wav</td>\n",
       "      <td>पेट</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0116_003_segment_7.wav</td>\n",
       "      <td>गरमगरम</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0116_003_segment_9.wav</td>\n",
       "      <td>हड़पते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0128_003_segment_0.wav</td>\n",
       "      <td>मुनिया</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              audio_path transcript\n",
       "0           1  0116_003_segment_1.wav       अपने\n",
       "1           2  0116_003_segment_2.wav        पेट\n",
       "2           7  0116_003_segment_7.wav     गरमगरम\n",
       "3           9  0116_003_segment_9.wav      हड़पते\n",
       "4          10  0128_003_segment_0.wav     मुनिया"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import pandas as pd \n",
    "import random \n",
    "import os \n",
    "\n",
    "def edit_distance(word1,word2):\n",
    "    distance=Levenshtein.distance(word1,word2)\n",
    "    return distance\n",
    "\n",
    "def sample_words_with_lev_scores(df, lev_score, word):\n",
    "    filtered_words = df[df.apply(lambda row: edit_distance(row['transcript'], word), axis=1) == lev_score]\n",
    "    \n",
    "    if len(filtered_words) >= 2:\n",
    "        sampled_words = random.sample(list(filtered_words['transcript']), 2)\n",
    "    else:\n",
    "        sampled_words = list(filtered_words['transcript'])\n",
    "    \n",
    "    return sampled_words\n",
    "\n",
    "def sample_dev_words(path):\n",
    "    df=pd.read_csv(path)\n",
    "    sampled_list=[]\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        out_dict={}\n",
    "        for j in range(len(df)):\n",
    "            lev_distance=edit_distance(df['transcript'][i],df['transcript'][j])\n",
    "            if lev_distance in out_dict:\n",
    "                out_dict[lev_distance].append(df['transcript'][j])\n",
    "            else:\n",
    "                out_dict[lev_distance]=[df['transcript'][j]]\n",
    "        \n",
    "        sampled_list.append(out_dict)\n",
    "    \"\"\"\"for i in range(len(df)):\n",
    "        score=0\n",
    "        out_dict={}\n",
    "        count_list=[]\n",
    "        while len(count_list)<8:\n",
    "            lev_score_i_words = sample_words_with_lev_scores(df, score, df['transcript'][i])\n",
    "            out_dict[score]=lev_score_i_words\n",
    "            \n",
    "            count_list+=lev_score_i_words\n",
    "            score+=1\n",
    "\n",
    "        sampled_list.append(out_dict)\"\"\"\n",
    "\n",
    "    df['sampled_words']=sampled_list\n",
    "\n",
    "    root_path=path.split('dataset/')[0] + 'dataset/'\n",
    "    save_path=os.path.join(root_path,\"sampled_devset.csv\")\n",
    "\n",
    "    df.to_csv(save_path)\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampler import sample_negatives\n",
    "\n",
    "sampled_trainset=sample_negatives('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/train_set.csv')\n",
    "sampled_dev_set=sample_dev_words('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/dev_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random, time, operator \n",
    "import os \n",
    "import torch \n",
    "from utils import _load_vocab\n",
    "from utils import load_audio\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import librosa\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import librosa.display\n",
    "import ast \n",
    "\n",
    "VOCAB_DICT=_load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiviewDevDataset(Dataset):\n",
    "\n",
    "    def __init__(self,csv_file,n_mfcc=13):\n",
    "        self.data=pd.read_csv(csv_file)\n",
    "        self.dir_path=os.path.dirname(csv_file)\n",
    "        self.vocab_dict=VOCAB_DICT\n",
    "        self.n_mfcc=n_mfcc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def char_to_idx(self,transcript):\n",
    "        \n",
    "        one_hot=torch.zeros(len(transcript),len(self.vocab_dict))\n",
    "        for i,char in enumerate(transcript):\n",
    "            one_hot[i,self.vocab_dict[char]]=1 \n",
    "        \n",
    "        return one_hot\n",
    "    \n",
    "    def compute_mfcc(self,audio_path):\n",
    "\n",
    "        y,sr=librosa.load(audio_path)\n",
    "\n",
    "        n_fft = min(2048, len(y))\n",
    "        hop_length = n_fft // 4\n",
    "\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "        width = min(9, mfccs.shape[1])\n",
    "        if width < 3:\n",
    "            width = 3\n",
    "        \n",
    "        width = min(width, mfccs.shape[1])\n",
    "\n",
    "        if width % 2 == 0:\n",
    "            width -= 1\n",
    "\n",
    "        \n",
    "        delta1=librosa.feature.delta(mfccs,order=1,width=width)\n",
    "        delta2=librosa.feature.delta(mfccs,order=2,width=width)\n",
    "\n",
    "        mfccs_combined=np.concatenate((mfccs,delta1,delta2),axis=0)\n",
    "\n",
    "        return mfccs_combined\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        audio_path_x1=self.data[\"audio_path\"][idx]\n",
    "        audio_path_x1=os.path.join(self.dir_path,str(audio_path_x1))\n",
    "\n",
    "        #mfcc \n",
    "        audio_mfcc=self.compute_mfcc(audio_path_x1)\n",
    "\n",
    "        sample_dict=ast.literal_eval(self.data[\"sampled_words\"][idx])\n",
    "        lev_scores=[]\n",
    "        for score in sample_dict.keys():\n",
    "            for _ in range(len(sample_dict[score])):\n",
    "                lev_scores.append(score)\n",
    "\n",
    "        one_hot=[]\n",
    "        for transcripts in sample_dict.values():\n",
    "            for transcript in transcripts:\n",
    "                one_hot.append(self.char_to_idx(transcript))\n",
    "        \n",
    "        output_tensor=[torch.tensor(audio_mfcc),one_hot,torch.tensor(lev_scores)]\n",
    "\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset=MultiviewDevDataset('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/sampled_devset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 6, 1, 8, 9])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset[0][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, batch_first=True, padding_value=0):\n",
    "    return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=batch_first, padding_value=padding_value)\n",
    "\n",
    "def pad_mfccs(mfccs, max_len):\n",
    "    padded_mfccs = []\n",
    "    for mfcc in mfccs:\n",
    "        # Padding to the right with zeros\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        padded_mfcc = torch.nn.functional.pad(mfcc, (0, pad_width), 'constant', 0)\n",
    "        padded_mfccs.append(padded_mfcc)\n",
    "    return torch.stack(padded_mfccs)\n",
    "\n",
    "def pad_list_of_lists(batch_of_sequences, padding_value=0):\n",
    "    padded_batch = []\n",
    "    for sequences in batch_of_sequences:\n",
    "        sequences = [torch.tensor(seq) for seq in sequences]\n",
    "        padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=padding_value)\n",
    "        padded_batch.append(padded_sequences)\n",
    "    \n",
    "    return torch.stack(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tensor=pad_sequence(dev_dataset[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 9, 83])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_collate_fn(batch):\n",
    "    \n",
    "    mfccs_x1=[]\n",
    "    one_hot=[]\n",
    "    lev_scores=[]\n",
    "\n",
    "    for item in batch:\n",
    "        mfcc_x1,oh,lev_score=item[0],item[1],item[2]\n",
    "        mfccs_x1.append(mfcc_x1)\n",
    "        one_hot.append(oh)\n",
    "        lev_scores.append(lev_score)\n",
    "    \n",
    "    max_mfcc_len_x1=max(mfcc.shape[1] for mfcc in mfccs_x1)\n",
    "    mfccs_x1=pad_mfccs(mfccs_x1,max_mfcc_len_x1)\n",
    "\n",
    "    one_hot=pad_list_of_lists(one_hot)\n",
    "\n",
    "    results={\"mfcc\":mfccs_x1,\"sampled_one_hot\":one_hot,\"lev_scores\":torch.stack(lev_scores)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader=DataLoader(dev_dataset, batch_size=2, collate_fn=dev_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_608917/1187922984.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequences = [torch.tensor(seq) for seq in sequences]\n"
     ]
    }
   ],
   "source": [
    "batches=iter(dev_loader)\n",
    "batch=next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 9, 83])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"sampled_one_hot\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import _load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MultiViewRNN\n",
    "\n",
    "config_file=_load_config()\n",
    "model=MultiViewRNN(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiViewRNN(\n",
       "  (net): ModuleDict(\n",
       "    (view1): RNN_default(\n",
       "      (rnn): LSTM(39, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    )\n",
       "    (view2): RNN_default(\n",
       "      (rnn): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor=batch[\"sampled_one_hot\"]\n",
    "input_tensor=input_tensor.view(input_tensor.shape[0]*18,input_tensor.shape[2],input_tensor.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 9, 83])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_one_hot={\"view2_c1\":input_tensor}\n",
    "out_one_hot=model(input_one_hot)[\"c1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 1024])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_one_hot=out_one_hot.view(2,18,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 1024])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_one_hot.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc=batch[\"mfcc\"]\n",
    "mfcc=mfcc.view(-1,mfcc.shape[2],mfcc.shape[1])\n",
    "mfcc_input={\"view1_x1\":mfcc}\n",
    "audio_emb=model(mfcc_input)[\"x1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_text_emb=out_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 1024])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_text_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_emb=audio_emb.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_emb=audio_emb.squeeze(1)\n",
    "audio_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import ranked_batch_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1024])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "normalized_audio_embeddings = F.normalize(audio_emb, p=2, dim=1)  \n",
    "normalized_text_embeddings = F.normalize(sampled_text_emb, p=2, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_audio_embeddings = normalized_audio_embeddings.unsqueeze(1)\n",
    "cosine_similarities = torch.sum(expanded_audio_embeddings * normalized_text_embeddings, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=torch.argsort(cosine_similarities,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  5,  6, 16,  1, 14,  4,  7, 15,  8,  2, 11,  3, 13, 17,  0, 10,  9],\n",
       "        [11,  7,  6,  9, 10,  8, 14, 15,  2,  3,  1, 17,  5,  0,  4, 16, 12, 13]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_score=torch.stack(batch[\"lev_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap=ranked_batch_ap(lev_score,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24358975887298584"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from dataset import MultiviewDevDataset\n",
    "dataset_path='/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/train_aligned_dataset/sampled_devset.csv'\n",
    "dataset=MultiviewDevDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0][1][19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "batch=[[torch.randn(2,83),torch.randn(3,83)],[torch.randn(2,83),torch.randn(4,83)],[torch.randn(1,83),torch.randn(5,83)]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_batch_sequence(batch, padding_value=0):\n",
    "    padded_batch = []\n",
    "\n",
    "    max_seq_length = max(len(seq) for sequences in batch for seq in sequences)\n",
    "    max_num_sequences = max(len(sequences) for sequences in batch)\n",
    "    \n",
    "    for sequences in batch:\n",
    "        padded_sequences = [\n",
    "            torch.nn.functional.pad(torch.tensor(seq), (0, 0, 0, max_seq_length - len(seq)), 'constant', padding_value)\n",
    "            for seq in sequences\n",
    "        ]\n",
    "        \n",
    "        padded_sequences = pad_sequence(padded_sequences, batch_first=True, padding_value=padding_value)\n",
    "        \n",
    "        if len(padded_sequences) < max_num_sequences:\n",
    "            padding = torch.full((max_num_sequences - len(padded_sequences), max_seq_length, padded_sequences.shape[2]), padding_value)\n",
    "            padded_sequences = torch.cat((padded_sequences, padding), dim=0)\n",
    "        \n",
    "        padded_batch.append(padded_sequences)\n",
    "    \n",
    "    stacked_tensor = torch.stack(padded_batch)\n",
    "\n",
    "    return stacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24716/3851736655.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.nn.functional.pad(torch.tensor(seq), (0, 0, 0, max_seq_length - len(seq)), 'constant', padding_value)\n"
     ]
    }
   ],
   "source": [
    "padded=pad_batch_sequence(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 5, 83])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_dev_loader\n",
    "\n",
    "dev_loader=get_dev_loader('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/train_aligned_dataset/sampled_devset.csv',32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=iter(dev_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.nn.functional.pad(torch.tensor(seq), (0, 0, 0, max_seq_length - len(seq)), 'constant', padding_value)\n"
     ]
    }
   ],
   "source": [
    "batch_element=next(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_element['lev_scores'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MultiViewRNN\n",
    "from utils import _load_config\n",
    "config_file=_load_config()\n",
    "model=MultiViewRNN(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc=batch_element[\"mfcc\"]\n",
    "mfcc=mfcc.view(-1,mfcc.shape[2],mfcc.shape[1])\n",
    "mfcc_input={\"view1_x1\":mfcc}\n",
    "audio_emb=model(mfcc_input)[\"x1\"] \n",
    "\n",
    "input_text_tensor=batch_element[\"sampled_one_hot\"]\n",
    "batch_size=input_text_tensor.shape[0]\n",
    "sampled_shape=input_text_tensor.shape[1]\n",
    "input_text_tensor=input_text_tensor.view(input_text_tensor.shape[0]*sampled_shape,\n",
    "                                                        input_text_tensor.shape[2],\n",
    "                                                        input_text_tensor.shape[3])\n",
    "input_one_hot={\"view2_c1\":input_text_tensor}\n",
    "out_one_hot=model(input_one_hot)[\"c1\"]\n",
    "text_emb=out_one_hot.view(batch_size,\n",
    "                                    sampled_shape,\n",
    "                                    out_one_hot.shape[1])\n",
    "lev_distances=batch_element[\"lev_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 1024])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def crossview_ap(audio_embedding,text_embedding,lev_distances):\\n\\n    indices=get_indices(audio_embedding=audio_embedding,text_embedding=text_embedding)\\n    \\n    average_precission=ranked_batch_ap(lev_distances,indices)\\n\\n    return average_precission\\n\\ndef ranked_batch_ap(lev_distances, cosine_ranks):\\n    \\n    relevant_ranks = cosine_ranks.masked_select(lev_distances == 0).sort()[0]\\n    device=relevant_ranks.device\\n\\n    pos_indices = torch.arange(1, relevant_ranks.size(0) + 1,device=device).float()\\n\\n    precision_at_k = pos_indices / (relevant_ranks.float() + 1)\\n\\n    batch_ap = precision_at_k.sum() / relevant_ranks.size(0)\\n    return batch_ap.item()'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  \n",
    "import torch.nn.functional as F \n",
    "\n",
    "def get_indices(audio_embedding,text_embedding):\n",
    "    \n",
    "    normalized_audio_embeddings = F.normalize(audio_embedding, p=2, dim=1)\n",
    "    normalized_text_embeddings = F.normalize(text_embedding, p=2, dim=2)\n",
    "\n",
    "    expanded_audio_embeddings = normalized_audio_embeddings.unsqueeze(1)\n",
    "\n",
    "    cosine_similarities = torch.sum(expanded_audio_embeddings * normalized_text_embeddings, dim=2)\n",
    "\n",
    "    del normalized_audio_embeddings\n",
    "    del normalized_text_embeddings\n",
    "    del expanded_audio_embeddings\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "    indices=torch.argsort(cosine_similarities,dim=1)\n",
    "\n",
    "    del cosine_similarities\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return indices \n",
    "\n",
    "\"\"\"def crossview_ap(audio_embedding,text_embedding,lev_distances):\n",
    "\n",
    "    indices=get_indices(audio_embedding=audio_embedding,text_embedding=text_embedding)\n",
    "    \n",
    "    average_precission=ranked_batch_ap(lev_distances,indices)\n",
    "\n",
    "    return average_precission\n",
    "\n",
    "def ranked_batch_ap(lev_distances, cosine_ranks):\n",
    "    \n",
    "    relevant_ranks = cosine_ranks.masked_select(lev_distances == 0).sort()[0]\n",
    "    device=relevant_ranks.device\n",
    "\n",
    "    pos_indices = torch.arange(1, relevant_ranks.size(0) + 1,device=device).float()\n",
    "\n",
    "    precision_at_k = pos_indices / (relevant_ranks.float() + 1)\n",
    "\n",
    "    batch_ap = precision_at_k.sum() / relevant_ranks.size(0)\n",
    "    return batch_ap.item()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn.functional as F \n",
    "\n",
    "def crossview_ap(audio_embedding,text_embedding,lev_distances):\n",
    "\n",
    "    normalized_audio_embeddings = F.normalize(audio_embedding, p=2, dim=1)\n",
    "    normalized_text_embeddings = F.normalize(text_embedding, p=2, dim=2)\n",
    "\n",
    "    expanded_audio_embeddings = normalized_audio_embeddings.unsqueeze(1)\n",
    "    \n",
    "    cosine_similarities = torch.sum(expanded_audio_embeddings * normalized_text_embeddings, dim=2)\n",
    "\n",
    "    #freeing the memory \n",
    "    del normalized_audio_embeddings\n",
    "    del normalized_text_embeddings\n",
    "    del expanded_audio_embeddings\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "    indices=torch.argsort(cosine_similarities,dim=1)\n",
    "\n",
    "    average_precission=ranked_batch_ap(lev_distances,indices)\n",
    "\n",
    "    del cosine_similarities\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "    return average_precission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_ap=crossview_ap(audio_embedding=audio_emb,text_embedding=text_emb,lev_distances=lev_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13431194424629211"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=get_indices(audio_embedding=audio_emb,text_embedding=text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15,  5, 16,  3, 17,  9,  1,  0, 12, 13, 19, 11, 14,  6,  2, 10,  8, 18,\n",
       "          7,  4],\n",
       "        [17, 14, 12, 19,  5, 15,  3,  9,  8, 18,  6, 11,  2, 10, 16, 13,  4,  1,\n",
       "          0,  7],\n",
       "        [13, 17,  0,  3, 14,  6,  5,  7,  2, 19, 16,  1, 18,  9, 15,  8, 12,  4,\n",
       "         10, 11],\n",
       "        [ 2,  0, 11,  4,  7, 13,  5, 10, 12,  6, 18,  8, 14,  1, 19, 15, 16, 17,\n",
       "          3,  9],\n",
       "        [10, 18, 17,  9, 14,  7, 19,  1,  5,  0,  4, 15, 12, 16,  3,  2, 11, 13,\n",
       "          8,  6],\n",
       "        [15, 13,  7,  6,  9, 17,  5, 18,  3, 16,  8,  4,  2, 12,  1, 14, 10,  0,\n",
       "         19, 11],\n",
       "        [ 5,  1, 14, 18,  6, 19,  8, 13,  7, 10,  4,  2,  3,  0, 15,  9, 17, 16,\n",
       "         11, 12],\n",
       "        [11,  3,  2, 16,  0,  9,  7,  5, 15, 12, 17,  8,  1,  6, 10, 13,  4, 19,\n",
       "         14, 18],\n",
       "        [14, 13, 17,  3,  6, 11,  1,  5,  0, 19,  8,  7, 18,  2,  9,  4, 10, 12,\n",
       "         15, 16],\n",
       "        [13,  3, 17, 12,  2, 15, 16, 18,  6,  5, 14,  4,  1, 11, 10,  0, 19,  9,\n",
       "          8,  7],\n",
       "        [14, 16, 10, 19,  8,  3,  5,  4, 11, 15,  1, 18,  6,  0,  2, 12, 17,  7,\n",
       "         13,  9],\n",
       "        [ 6, 15, 16, 13, 19,  9,  8,  5,  4,  7, 18,  0, 14, 10,  2,  1,  3, 17,\n",
       "         11, 12],\n",
       "        [ 6,  0,  1,  2, 16, 17, 12, 15, 14,  9,  8, 19,  3, 11, 18,  4, 13,  5,\n",
       "          7, 10],\n",
       "        [12,  8, 11, 15,  4, 19,  9,  3,  7,  2, 14, 17, 18, 16,  1, 10, 13,  0,\n",
       "          5,  6],\n",
       "        [16, 19, 10, 11,  1,  2,  5,  0,  3, 17,  4,  7,  8, 15,  9,  6, 14, 12,\n",
       "         13, 18],\n",
       "        [12,  2, 15,  3, 11, 19,  5,  4, 17,  0, 18,  9,  1, 13,  6,  8, 14, 10,\n",
       "         16,  7],\n",
       "        [16, 12, 14, 11,  8,  3,  1, 18, 13,  5,  0,  2, 15,  9,  6, 10, 19,  4,\n",
       "         17,  7],\n",
       "        [13,  6,  0, 18, 11,  4, 19,  8, 17,  9, 16,  1, 15, 14, 12,  2,  5,  3,\n",
       "          7, 10],\n",
       "        [16, 12,  5, 19,  4,  8,  6,  3, 18, 13,  7, 11,  2,  9, 17, 15,  0, 10,\n",
       "          1, 14],\n",
       "        [18,  6,  4,  1, 19,  0,  3, 15,  2, 14, 10, 16, 11, 12,  7,  9,  5, 17,\n",
       "          8, 13],\n",
       "        [12,  5, 11,  7,  0, 17, 16,  6, 10,  9,  4, 14, 13,  3, 19,  8,  1, 18,\n",
       "         15,  2],\n",
       "        [18,  5,  9, 14,  0,  4, 17, 11, 10,  8,  1,  7, 12, 13, 16,  6, 19,  3,\n",
       "          2, 15],\n",
       "        [10, 16, 11,  8, 15,  9, 17, 13, 19, 12,  2,  0,  7, 18,  3,  4,  1,  6,\n",
       "         14,  5],\n",
       "        [12, 14,  6, 13,  9, 17,  7,  4,  8, 19, 18, 10,  5, 15,  0,  3, 16, 11,\n",
       "          2,  1],\n",
       "        [10,  7, 17, 11,  8,  2, 19, 13,  5, 14, 16,  4,  0, 18, 12, 15,  3,  6,\n",
       "          1,  9],\n",
       "        [15, 12,  0, 19,  1,  5, 16, 18,  6, 11, 10,  4, 14,  3,  8,  9, 17, 13,\n",
       "          7,  2],\n",
       "        [10,  2, 14, 18, 15,  4,  9, 13,  0, 17, 11,  6,  3,  5, 12,  7, 16, 19,\n",
       "          1,  8],\n",
       "        [14,  3, 19, 11,  6,  8,  5,  7, 18, 17,  9,  2,  1, 12,  4,  0, 13, 15,\n",
       "         16, 10],\n",
       "        [19, 10, 13, 16,  5, 15,  2, 11,  0, 12,  4,  3, 14,  1,  9,  6, 17,  8,\n",
       "          7, 18],\n",
       "        [19, 18,  1, 11,  0, 16,  2, 17,  3,  4,  8,  6,  9,  5, 14, 10,  7, 13,\n",
       "         12, 15],\n",
       "        [ 3,  1,  5, 15, 13,  6, 14, 10,  8,  7, 12, 19, 17,  9,  2,  4, 18,  0,\n",
       "         11, 16],\n",
       "        [14,  9, 12,  0,  4, 10, 19, 16, 13, 15, 17,  8,  6, 11, 18,  2,  5,  1,\n",
       "          7,  3]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5],\n",
       "        [0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "        [0, 0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "        [0, 0, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "        [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "        [0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_ranks = indices.masked_select(lev_distances == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15,  5, 17, 14, 13, 17,  2, 10, 15, 13,  5, 11,  3, 14, 13,  3, 14, 16,\n",
       "         6,  6, 12,  8, 16, 19, 12,  2, 16, 12, 13, 16, 12, 18,  6, 12,  5, 18,\n",
       "        10, 16, 12, 14, 10,  7, 15, 12, 10,  2, 14,  3, 19, 10, 19,  3, 14])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_ranks.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_indices=torch.arange(1,relevant_ranks.size(0)+1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n",
       "        15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28.,\n",
       "        29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42.,\n",
       "        43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ranks=indices[0].masked_select(lev_distances[0]==0).sort(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_indices=torch.arange(1,rel_ranks.size(0)+1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_k = pos_indices / (rel_ranks.float() + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1667, 0.1250])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_batch_ap(lev_distances, cosine_ranks):\n",
    "\n",
    "    batch_ap=0.0\n",
    "    num_elements=lev_distances.size(0)\n",
    "\n",
    "    for i in range(num_elements):\n",
    "        \n",
    "        relevant_ranks=cosine_ranks[i].masked_select(lev_distances[i]==0).sort()[0]\n",
    "        if relevant_ranks.numel()==0:\n",
    "            continue \n",
    "\n",
    "        pos_indices=torch.arange(1,relevant_ranks.size(0)+1,device=relevant_ranks.device).float()\n",
    "        precision_at_k=pos_indices/(relevant_ranks+1)\n",
    "\n",
    "        average_precission_i=precision_at_k.sum()/relevant_ranks.size(0)\n",
    "        batch_ap+=average_precission_i\n",
    "    \n",
    "    if num_elements>0:\n",
    "        batch_ap/=num_elements\n",
    "    \n",
    "    return batch_ap.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
