{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "import os \n",
    "import torch \n",
    "from utils import _load_vocab,_load_config\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import librosa\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import librosa.display\n",
    "import ast\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_DICT=_load_vocab()\n",
    "config_file=_load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self,csv_file,loss_fn,n_mfcc=13):\n",
    "        self.data=pd.read_csv(csv_file)\n",
    "        self.dir_path=os.path.dirname(csv_file)\n",
    "        self.vocab_dict=VOCAB_DICT\n",
    "        self.n_mfcc=n_mfcc\n",
    "        self.loss_fn=loss_fn \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def char_to_idx(self,transcript):\n",
    "        \n",
    "        one_hot=torch.zeros(len(transcript),len(self.vocab_dict))\n",
    "        for i,char in enumerate(transcript):\n",
    "            one_hot[i,self.vocab_dict[char]]=1 \n",
    "        \n",
    "        return one_hot\n",
    "    \n",
    "    def compute_mfcc(self,audio_path):\n",
    "\n",
    "        y,sr=librosa.load(audio_path)\n",
    "\n",
    "        n_fft = min(2048, len(y))\n",
    "        hop_length = n_fft // 4\n",
    "\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "        width = min(9, mfccs.shape[1])\n",
    "        if width < 3:\n",
    "            width = 3\n",
    "        \n",
    "        width = min(width, mfccs.shape[1])\n",
    "\n",
    "        if width % 2 == 0:\n",
    "            width -= 1\n",
    "\n",
    "        \n",
    "        delta1=librosa.feature.delta(mfccs,order=1,width=width)\n",
    "        delta2=librosa.feature.delta(mfccs,order=2,width=width)\n",
    "\n",
    "        mfccs_combined=np.concatenate((mfccs,delta1,delta2),axis=0)\n",
    "\n",
    "        return mfccs_combined\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        audio_path_x1=self.data[\"audio_path\"][idx]\n",
    "        audio_path_x1=os.path.join(self.dir_path,str(audio_path_x1))\n",
    "        transcript_c1=self.data[\"transcript\"][idx]\n",
    "\n",
    "        sample_list=ast.literal_eval(self.data[\"negative_samples\"][idx])\n",
    "        \n",
    "        one_hot_c2=None\n",
    "        if 0 in self.loss_fn or 1 in self.loss_fn:\n",
    "            transcript_c2=random.choice(sample_list)[1]\n",
    "            one_hot_c2=self.char_to_idx(transcript_c2)\n",
    "        \n",
    "        mfccs_x2=None\n",
    "        if 2 in self.loss_fn or 3 in self.loss_fn:\n",
    "            audio_path_x2=random.choice(sample_list)[0]\n",
    "            audio_path_x2=os.path.join(self.dir_path,str(audio_path_x2))\n",
    "\n",
    "            mfccs_x2=self.compute_mfcc(audio_path_x2)\n",
    "        \n",
    "        #computing mfcc\n",
    "        mfcc_x1=self.compute_mfcc(audio_path_x1) \n",
    "        \n",
    "        #computiing one hot for transcript \n",
    "        one_hot_c1=self.char_to_idx(transcript_c1)\n",
    "\n",
    "        output_tensors= [torch.tensor(mfcc_x1), one_hot_c1]\n",
    "\n",
    "        if 0 in self.loss_fn or 1 in self.loss_fn:\n",
    "            output_tensors.append(one_hot_c2)\n",
    "    \n",
    "        if 2 in self.loss_fn or 3 in self.loss_fn:\n",
    "            output_tensors.append(torch.tensor(mfccs_x2))\n",
    "    \n",
    "        return output_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_mfccs(mfccs, max_len):\n",
    "    padded_mfccs = []\n",
    "    for mfcc in mfccs:\n",
    "        # Padding to the right with zeros\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        padded_mfcc = torch.nn.functional.pad(mfcc, (0, pad_width), 'constant', 0)\n",
    "        padded_mfccs.append(padded_mfcc)\n",
    "    return torch.stack(padded_mfccs)\n",
    "\n",
    "def pad_sequence(sequences, batch_first=False, padding_value=0):\n",
    "    return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=batch_first, padding_value=padding_value)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    mfccs_x1= []\n",
    "    one_hot_c1=[]\n",
    "    one_hot_c2=[]\n",
    "    mfccs_x2=[]\n",
    "\n",
    "    for item in batch:\n",
    "        mfcc_x1,oh_c1 = item[0], item[1]\n",
    "        mfccs_x1.append(mfcc_x1)\n",
    "        one_hot_c1.append(oh_c1)\n",
    "\n",
    "        if len(item) == 4:\n",
    "            oh_c2,mfcc_x2= item[2], item[3]\n",
    "            one_hot_c2.append(oh_c2)\n",
    "            mfccs_x2.append(mfcc_x2)\n",
    "        \n",
    "        elif len(item) == 3:\n",
    "            if item[2].shape[1] == len(VOCAB_DICT):\n",
    "                oh_c2=item[2]\n",
    "                one_hot_c2.append(oh_c2)\n",
    "            else:\n",
    "                mfcc_x2=item[2]\n",
    "                mfccs_x2.append(mfcc_x2)\n",
    "    \n",
    "    max_mfcc_len_x1=max(mfcc.shape[1] for mfcc in mfccs_x1)\n",
    "    max_mfcc_len_x2=max(mfcc.shape[1] for mfcc in mfccs_x2) if mfccs_x2 else 0\n",
    "\n",
    "    mfccs_x1=pad_mfccs(mfccs_x1,max_mfcc_len_x1)\n",
    "    one_hot_c1=pad_sequence(one_hot_c1, batch_first=True)\n",
    "\n",
    "    result={'view1_x1':mfccs_x1,'view2_c1':one_hot_c1}\n",
    "\n",
    "    if one_hot_c2:\n",
    "        one_hot_c2=pad_sequence(one_hot_c2,batch_first=True)\n",
    "        result['view2_c2']=one_hot_c2\n",
    "    \n",
    "    if mfccs_x2:\n",
    "        mfccs_x2=pad_mfccs(mfccs_x2, max_mfcc_len_x2)\n",
    "        result['view1_x2']=mfccs_x2\n",
    "    \n",
    "    return result \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file='/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/sampled_metadata.csv'\n",
    "dataset=AudioDataset(csv_file=csv_file,loss_fn=config_file[\"loss_fn\"])\n",
    "\n",
    "dataloader=DataLoader(dataset, batch_size=3, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=iter(dataloader)\n",
    "\n",
    "batch=next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['view1_x1', 'view2_c1', 'view2_c2'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 45, 39])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['view1_x1']=batch['view1_x1'].view(3,45,39)\n",
    "batch['view1_x1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 83])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['view2_c1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 83])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['view2_c2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MultiViewRNN\n",
    "model=MultiViewRNN(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiViewRNN(\n",
       "  (net): ModuleDict(\n",
       "    (view1): RNN_default(\n",
       "      (rnn): LSTM(39, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    )\n",
       "    (view2): RNN_default(\n",
       "      (rnn): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x2', 'c2', 'x1', 'c1'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['x1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['c1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['c2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import contrastive_loss\n",
    "loss=contrastive_loss(obj=config_file[\"loss_fn\"],margin=config_file[\"margin\"],x1=out['x1'],c1=out['c1'],c2=out['c2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5061, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from itertools import combinations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=pd.read_csv('/home/ubuntu/acoustic_stuff/hindi-acoustic-word-embedding/dataset/sampled_metadata.csv')\n",
    "grouped = metadata.groupby('transcript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_list=list(grouped['audio_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('अपने',\n",
       "  1    0116_003_segment_1.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('अप्पा',\n",
       "  42    0139_003_segment_7.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('उन्हें',\n",
       "  12    0128_003_segment_2.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('उसे',\n",
       "  52    0153_003_segment_3.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('एक',\n",
       "  30    0136_003_segment_8.wav\n",
       "  53    0153_003_segment_4.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('और',\n",
       "  0      0116_003_segment_0.wav\n",
       "  32    0136_003_segment_10.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('कर',\n",
       "  18    0128_003_segment_8.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('कहा',\n",
       "  39    0139_003_segment_4.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('की',\n",
       "  5     0116_003_segment_5.wav\n",
       "  15    0128_003_segment_5.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('के',\n",
       "  43    0139_003_segment_8.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('को',\n",
       "  3    0116_003_segment_3.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('कोलम',\n",
       "  54    0153_003_segment_5.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('गयीं',\n",
       "  29    0136_003_segment_7.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('गरमगरम',\n",
       "  7    0116_003_segment_7.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('गोल',\n",
       "  34    0136_003_segment_12.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('जलेबियाँ',\n",
       "  8    0116_003_segment_8.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('जाता',\n",
       "  57    0153_003_segment_8.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('जाते',\n",
       "  19    0128_003_segment_9.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('तैरती',\n",
       "  26    0136_003_segment_4.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('तो',\n",
       "  41    0139_003_segment_6.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('था',\n",
       "  58    0153_003_segment_9.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('दिया',\n",
       "  56    0153_003_segment_7.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('दूसरी',\n",
       "  33    0136_003_segment_11.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('देखा',\n",
       "  21    0128_003_segment_11.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('दो',\n",
       "  22    0136_003_segment_0.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('निकल',\n",
       "  28    0136_003_segment_6.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('ने',\n",
       "  11    0128_003_segment_1.wav\n",
       "  36    0139_003_segment_1.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('पकड़ने',\n",
       "  14    0128_003_segment_4.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('पतली',\n",
       "  31    0136_003_segment_9.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('पर',\n",
       "  51    0153_003_segment_2.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('पेट',\n",
       "  2    0116_003_segment_2.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('पैरों',\n",
       "  44    0139_003_segment_9.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('पोंगल',\n",
       "  50    0153_003_segment_1.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('बंसीे',\n",
       "  16    0128_003_segment_6.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('बनाने',\n",
       "  55    0153_003_segment_6.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('बड़ी',\n",
       "  47    0139_003_segment_12.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('भी',\n",
       "  46    0139_003_segment_11.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('मछलियाँ',\n",
       "  23    0136_003_segment_1.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('मछली',\n",
       "  13    0128_003_segment_3.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('माँ',\n",
       "  4    0116_003_segment_4.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('मुनिया',\n",
       "  10    0128_003_segment_0.wav\n",
       "  35    0139_003_segment_0.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('यह',\n",
       "  40    0139_003_segment_5.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('ले',\n",
       "  17    0128_003_segment_7.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('सामने',\n",
       "  24    0136_003_segment_2.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('से',\n",
       "  25     0136_003_segment_3.wav\n",
       "  45    0139_003_segment_10.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('स्वादिष्ट',\n",
       "  6    0116_003_segment_6.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('हँसते',\n",
       "  37    0139_003_segment_2.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('हर',\n",
       "  49    0153_003_segment_0.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('हुई',\n",
       "  27    0136_003_segment_5.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('हुए',\n",
       "  20    0128_003_segment_10.wav\n",
       "  38     0139_003_segment_3.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('है',\n",
       "  48    0139_003_segment_13.wav\n",
       "  Name: audio_path, dtype: object),\n",
       " ('हड़पते',\n",
       "  9    0116_003_segment_9.wav\n",
       "  Name: audio_path, dtype: object)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_dict=dict(gr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words=metadata['transcript'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_pairs=combinations(metadata['audio_path'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.combinations at 0x7696c11eb9c0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'0116_003_segment_0.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m audio_pairs:\n\u001b[1;32m      2\u001b[0m     audio1,audio2\u001b[38;5;241m=\u001b[39mpair\n\u001b[0;32m----> 3\u001b[0m     transcript1\u001b[38;5;241m=\u001b[39m\u001b[43mgr_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43maudio1\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m     transcript2\u001b[38;5;241m=\u001b[39mgr_dict[audio2]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m transcript1\u001b[38;5;241m==\u001b[39mtranscript2:\n",
      "\u001b[0;31mKeyError\u001b[0m: '0116_003_segment_0.wav'"
     ]
    }
   ],
   "source": [
    "for pair in audio_pairs:\n",
    "    audio1,audio2=pair\n",
    "    transcript1=gr_dict[audio1].values[0]\n",
    "    transcript2=gr_dict[audio2].values[0]\n",
    "\n",
    "    if transcript1==transcript2:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "labels=np.array(labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
