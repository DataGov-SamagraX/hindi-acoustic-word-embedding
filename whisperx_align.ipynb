{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/suyash/acoustic_stuff/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/suyash/acoustic_stuff/venv/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import whisperx\n",
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda\"\n",
    "BATCH_SIZE=16\n",
    "COMPUTE_TYPE=\"float16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = pd.read_csv(dataframe)\n",
    "        self.dirname=os.path.dirname(dataframe)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_name = self.dataframe.iloc[idx]['audio_name']\n",
    "        audio_path=os.path.join(self.dirname,audio_name)\n",
    "\n",
    "        audio = whisperx.load_audio(audio_path)\n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Find the longest audio in the batch\n",
    "    max_length = max(audio.shape[0] for audio in batch)\n",
    "\n",
    "    # Create tensor for padded batch\n",
    "    padded_batch = torch.zeros((len(batch), max_length))\n",
    "\n",
    "    # Fill padded batch with audio data\n",
    "    for i, audio in enumerate(batch):\n",
    "        length = audio.shape[0]\n",
    "        padded_batch[i, :length] = torch.tensor(audio, dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "    return np.array(padded_batch)\n",
    "\n",
    "def create_dataloader(dataframe, batch_size):\n",
    "    dataset = AudioDataset(dataframe)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=create_dataloader(dataframe='/root/suyash/acoustic_stuff/hindi-acoustic-word-embedding/train_dataset/metadata.csv',batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.1+cu121. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "model=whisperx.load_model(\"whisper-tiny-ct2\", DEVICE, compute_type=COMPUTE_TYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/root/suyash/acoustic_stuff/hindi-acoustic-word-embedding/train_dataset/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path=os.path.dirname('/root/suyash/acoustic_stuff/hindi-acoustic-word-embedding/train_dataset/metadata.csv')\n",
    "audio_path=os.path.join(root_path,df['audio_name'][84])\n",
    "audio=whisperx.load_audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: audio is shorter than 30s, language detection may be inaccurate.\n",
      "Detected language: tg (0.11) in first 30s of audio...\n"
     ]
    }
   ],
   "source": [
    "result=model.transcribe(audio,batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at theainerd/Wav2Vec2-large-xlsr-hindi were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at theainerd/Wav2Vec2-large-xlsr-hindi and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_a, metadata = whisperx.load_align_model(language_code='hi', device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, DEVICE, return_char_alignments=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['segments', 'word_segments'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments=result['segments'][0]['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'जगदेव', 'start': 0.009, 'end': 1.073, 'score': 0.177},\n",
       " {'word': 'चंद', 'start': 1.213, 'end': 1.414, 'score': 0.327},\n",
       " {'word': 'ठाकुर', 'start': 1.494, 'end': 1.876, 'score': 0.411},\n",
       " {'word': 'की', 'start': 1.976, 'end': 2.056, 'score': 0.496},\n",
       " {'word': 'दो', 'start': 2.197, 'end': 2.337, 'score': 0.338},\n",
       " {'word': 'बेटियाँ', 'start': 2.418, 'end': 2.819, 'score': 0.546},\n",
       " {'word': 'विजिया', 'start': 2.839, 'end': 3.281, 'score': 0.271},\n",
       " {'word': 'पटियाल', 'start': 3.361, 'end': 3.863, 'score': 0.48},\n",
       " {'word': 'व', 'start': 3.983, 'end': 4.144, 'score': 0.248},\n",
       " {'word': 'सुनीता', 'start': 4.365, 'end': 4.686, 'score': 0.415},\n",
       " {'word': 'ठाकुर', 'start': 4.786, 'end': 5.107, 'score': 0.473},\n",
       " {'word': 'हैं।', 'start': 5.147, 'end': 5.208, 'score': 0.497}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['word_segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.073\n",
      "1 1.414\n",
      "2 1.876\n",
      "3 2.056\n",
      "4 2.337\n",
      "5 2.819\n",
      "6 3.281\n",
      "7 3.863\n",
      "8 4.144\n",
      "9 4.686\n",
      "10 5.107\n",
      "11 5.208\n"
     ]
    }
   ],
   "source": [
    "for i,seg in enumerate(segments):\n",
    "    print(i,seg['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
